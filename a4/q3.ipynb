{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jsonlines\n",
    "\n",
    "with jsonlines.open('data/train.jsonl') as reader:\n",
    "    train_data = [obj for obj in reader]\n",
    "\n",
    "with jsonlines.open('data/dev_seen.jsonl') as reader:\n",
    "    val_data = [obj for obj in reader]\n",
    "\n",
    "with jsonlines.open('data/test_seen.jsonl') as reader:\n",
    "    test_data = [obj for obj in reader]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "from transformers import BertModel, BertTokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "# Define the device to use\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define the hyperparameters\n",
    "batch_size = 32\n",
    "learning_rate = 0.0001\n",
    "num_epochs = 10\n",
    "\n",
    "# Define the dataset class\n",
    "\n",
    "\n",
    "class MemesDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, image_transform):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.image_transform = image_transform\n",
    "        self.img_dir = \"data/\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        img_path = os.path.join(self.img_dir, item['img'])\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        image = self.image_transform(image)\n",
    "        text = item['text']\n",
    "        tokenized_text = self.tokenizer.encode_plus(\n",
    "            text, add_special_tokens=True, max_length=128, padding='max_length', truncation=True)\n",
    "        input_ids = torch.tensor(tokenized_text['input_ids'])\n",
    "        attention_mask = torch.tensor(tokenized_text['attention_mask'])\n",
    "        label = torch.tensor(item['label'])\n",
    "        return {'image': image, 'input_ids': input_ids, 'attention_mask': attention_mask, 'label': label}\n",
    "\n",
    "# Define the image encoder\n",
    "\n",
    "\n",
    "class ImageEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ImageEncoder, self).__init__()\n",
    "        self.cnn = models.resnet50(pretrained=True)\n",
    "        self.cnn.fc = nn.Linear(self.cnn.fc.in_features, 1024)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.cnn(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        return x\n",
    "\n",
    "# Define the text encoder\n",
    "\n",
    "\n",
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TextEncoder, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs[1]\n",
    "        return pooled_output\n",
    "\n",
    "# Define the multimodal classifier\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "image_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "# Load the data\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "train_data = MemesDataset(train_data, tokenizer, image_transform)\n",
    "val_data = MemesDataset(val_data, tokenizer, image_transform)\n",
    "test_data = MemesDataset(test_data, tokenizer, image_transform)\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Train Loss: 0.5581\n",
      "Epoch [1/10], Val Loss: 0.7545, Val Acc: 0.5620\n",
      "Epoch [2/10], Train Loss: 0.4602\n",
      "Epoch [2/10], Val Loss: 1.0389, Val Acc: 0.5820\n",
      "Epoch [3/10], Train Loss: 0.3812\n",
      "Epoch [3/10], Val Loss: 0.8717, Val Acc: 0.5700\n",
      "Epoch [4/10], Train Loss: 0.3114\n",
      "Epoch [4/10], Val Loss: 1.2448, Val Acc: 0.5960\n",
      "Epoch [5/10], Train Loss: 0.2129\n",
      "Epoch [5/10], Val Loss: 1.5333, Val Acc: 0.5920\n",
      "Epoch [6/10], Train Loss: 0.1424\n",
      "Epoch [6/10], Val Loss: 1.4793, Val Acc: 0.5880\n",
      "Epoch [7/10], Train Loss: 0.1159\n",
      "Epoch [7/10], Val Loss: 1.8777, Val Acc: 0.5920\n",
      "Epoch [8/10], Train Loss: 0.0839\n",
      "Epoch [8/10], Val Loss: 2.2402, Val Acc: 0.5600\n",
      "Epoch [9/10], Train Loss: 0.0754\n",
      "Epoch [9/10], Val Loss: 2.1522, Val Acc: 0.5920\n",
      "Epoch [10/10], Train Loss: 0.0848\n",
      "Epoch [10/10], Val Loss: 2.2378, Val Acc: 0.5700\n"
     ]
    }
   ],
   "source": [
    "class MultimodalClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MultimodalClassifier, self).__init__()\n",
    "        self.image_encoder = ImageEncoder()\n",
    "        self.text_encoder = TextEncoder()\n",
    "        self.fc1 = nn.Linear(1792, 512)\n",
    "        self.fc2 = nn.Linear(512, 2)\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "\n",
    "    def forward(self, image, input_ids, attention_mask):\n",
    "        image_embedding = self.image_encoder(image)\n",
    "        text_embedding = self.text_encoder(input_ids, attention_mask)\n",
    "        multimodal_embedding = torch.cat(\n",
    "            (image_embedding, text_embedding), dim=1)\n",
    "        # print(multimodal_embedding.shape)\n",
    "        x = self.fc1(multimodal_embedding)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "model = MultimodalClassifier().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for batch in train_loader:\n",
    "        image = batch['image'].to(device)\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        label = batch['label'].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(image, input_ids, attention_mask)\n",
    "        loss = criterion(outputs, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * image.size(0)\n",
    "        \n",
    "    epoch_loss = running_loss / len(train_data)\n",
    "    print('Epoch [{}/{}], Train Loss: {:.4f}'.format(epoch +\n",
    "        1, num_epochs, epoch_loss))\n",
    "\n",
    "    # Evaluate the model on the validation set\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    num_correct = 0\n",
    "    num_total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            image = batch['image'].to(device)\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            label = batch['label'].to(device)\n",
    "\n",
    "            outputs = model(image, input_ids, attention_mask)\n",
    "            loss = criterion(outputs, label)\n",
    "\n",
    "            running_loss += loss.item() * image.size(0)\n",
    "            num_correct += (torch.argmax(outputs, axis=1) == label).sum().item()\n",
    "            num_total += label.size(0)\n",
    "\n",
    "    val_loss = running_loss / len(val_data)\n",
    "    val_acc = num_correct / num_total\n",
    "    print('Epoch [{}/{}], Val Loss: {:.4f}, Val Acc: {:.4f}'.format(epoch +\n",
    "        1, num_epochs, val_loss, val_acc))\n",
    "        \n",
    "    \n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
