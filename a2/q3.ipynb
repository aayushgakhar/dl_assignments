{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, SubsetRandomSampler, DataLoader\n",
    "from torchvision import datasets, models, transforms\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision.datasets import MNIST\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "import torch.nn as nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "classes = list(range(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the training dataset\n",
    "trainset = MNIST(root='data/', train=True,\n",
    "                      transform=ToTensor(), download=True)\n",
    "\n",
    "# Download the test dataset\n",
    "testset = MNIST(root='data/', train=False,\n",
    "                     transform=ToTensor(), download=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.8 * len(trainset))\n",
    "val_size = len(trainset) - train_size\n",
    "trainset, valset = torch.utils.data.random_split(trainset,\n",
    "                                                 [train_size, val_size])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    trainset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    valset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    testset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48000, 12000, 10000)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trainset), len(valset), len(testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = trainset.dataset.targets[trainset.indices].numpy()\n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "_idx, subset_idx = next(split.split(trainset.indices, labels))\n",
    "new_indices = list(np.array(trainset.indices)[subset_idx])\n",
    "subset_sampler = SubsetRandomSampler(new_indices)\n",
    "\n",
    "subset_loader = DataLoader(trainset.dataset, batch_size=64, sampler=subset_sampler)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48000, 9600, 150, 48000)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels), len(new_indices), len(subset_loader), len(trainset.indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels=1, out_channels=10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            in_channels=10, out_channels=20, kernel_size=2)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        \n",
    "        self.fc1 = nn.Linear(in_features=20*5*5, out_features=50)\n",
    "        self.fc2 = nn.Linear(in_features=50, out_features=10)\n",
    "\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "\n",
    "        x = x.flatten(start_dim=1)\n",
    "\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "def train(model, train_loader, subset_loader, transform, val_loader, criterion, optimizer, epochs=1):\n",
    "\n",
    "    train_loss = []\n",
    "    val_loss = []\n",
    "    train_acc = []\n",
    "    val_acc = []\n",
    "    t = tqdm(total=epochs*(len(train_loader)+len(subset_loader)), desc=\"Epoch 0/{}\".format(epochs))\n",
    "    for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "        t.set_description(\"Epoch {}/{}\".format(epoch, epochs))\n",
    "        t.refresh()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "\n",
    "        for i, (inputs, labels) in enumerate(train_loader, 0):\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            outputs = torch.argmax(outputs, dim=1)\n",
    "            correct += (outputs == labels).float().sum()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            t.update(1)\n",
    "        for i, (inputs, labels) in enumerate(subset_loader, 0):\n",
    "            inputs = transform(inputs)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            outputs = torch.argmax(outputs, dim=1)\n",
    "            correct += (outputs == labels).float().sum()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            t.update(1)\n",
    "\n",
    "        loss = running_loss / len(train_loader)\n",
    "        accuracy = 100*correct/len(train_loader.dataset)\n",
    "        train_loss.append(loss)\n",
    "        train_acc.append(accuracy)\n",
    "        correct = 0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for i, (inputs, labels) in enumerate(val_loader, 0):\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                outputs = torch.argmax(outputs, dim=1)\n",
    "                correct += (outputs == labels).float().sum()\n",
    "\n",
    "                running_loss += loss.item()\n",
    "            loss = running_loss / len(val_loader)\n",
    "            accuracy = 100*correct/len(val_loader.dataset)\n",
    "            val_loss.append(loss)\n",
    "            val_acc.append(accuracy)\n",
    "\n",
    "        model.train()\n",
    "    t.refresh()\n",
    "    t.close()\n",
    "\n",
    "    print('Finished Training')\n",
    "    return train_loss, val_loss, train_acc, val_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms_arr = [\n",
    "    transforms.Resize((32, 32)),\n",
    "    transforms.RandomHorizontalFlip(p=1),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.GaussianBlur(kernel_size=3, sigma=(0.1, 2.0)),\n",
    "]\n",
    "transforms_names = [\n",
    "    'Resize',\n",
    "    'LeftRightFlip',\n",
    "    'Rotation',\n",
    "    'GaussianNoise',\n",
    "]\n",
    "transforms_combined = torchvision.transforms.Compose(\n",
    "    transforms_arr+[transforms.ToTensor()])\n",
    "\n",
    "transforms_arr.append(transforms_combined)\n",
    "transforms_names.append('Combined')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "applying Resize\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (64x720 and 500x50)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mapplying\u001b[39m\u001b[39m'\u001b[39m,transform_name)\n\u001b[1;32m      3\u001b[0m model \u001b[39m=\u001b[39m CNN()\n\u001b[0;32m----> 4\u001b[0m train_loss, val_loss, train_acc, val_acc \u001b[39m=\u001b[39m train(model, train_loader, subset_loader, transform, val_loader, nn\u001b[39m.\u001b[39;49mCrossEntropyLoss(\n\u001b[1;32m      5\u001b[0m ), optim\u001b[39m.\u001b[39;49mAdam(model\u001b[39m.\u001b[39;49mparameters(), lr\u001b[39m=\u001b[39;49m\u001b[39m0.001\u001b[39;49m), epochs\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m)\n\u001b[1;32m      6\u001b[0m fig, axs \u001b[39m=\u001b[39m plt\u001b[39m.\u001b[39msubplots(\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m, figsize\u001b[39m=\u001b[39m(\u001b[39m10\u001b[39m, \u001b[39m5\u001b[39m), squeeze\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m      7\u001b[0m axs[\u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mplot(train_loss, label\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[29], line 73\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, subset_loader, transform, val_loader, criterion, optimizer, epochs)\u001b[0m\n\u001b[1;32m     70\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     72\u001b[0m \u001b[39m# forward + backward + optimize\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m outputs \u001b[39m=\u001b[39m model(inputs)\n\u001b[1;32m     75\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m     76\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/dl_assignments/.env/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[29], line 28\u001b[0m, in \u001b[0;36mCNN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     24\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpool(x)\n\u001b[1;32m     26\u001b[0m x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mflatten(start_dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m---> 28\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfc1(x)\n\u001b[1;32m     29\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu(x)\n\u001b[1;32m     30\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc2(x)\n",
      "File \u001b[0;32m~/dl_assignments/.env/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/dl_assignments/.env/lib/python3.8/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (64x720 and 500x50)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "for transform,transform_name in zip(transforms_arr, transforms_names):\n",
    "    print('applying',transform_name)\n",
    "    model = CNN()\n",
    "    train_loss, val_loss, train_acc, val_acc = train(model, train_loader, subset_loader, transform, val_loader, nn.CrossEntropyLoss(\n",
    "    ), optim.Adam(model.parameters(), lr=0.001), epochs=10)\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(10, 5), squeeze=False)\n",
    "    axs[0, 0].plot(train_loss, label='train')\n",
    "    axs[0, 0].plot(val_loss, label='val')\n",
    "    axs[0, 0].set_title('Loss vs Epochs')\n",
    "    axs[0, 0].legend()\n",
    "    # axs[0, 0].show()\n",
    "\n",
    "\n",
    "    axs[0, 1].plot(train_acc, label='train')\n",
    "    axs[0, 1].plot(val_acc, label='val')\n",
    "    axs[0, 1].set_title('Accuracy vs Epochs')\n",
    "    axs[0, 1].legend()\n",
    "    plt.show()\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3ff9dc6caf1e154d5edc32ebcbf2545cdc1e8c9297072b7268dfd34d4cb9c77b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
