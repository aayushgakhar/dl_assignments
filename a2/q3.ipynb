{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, SubsetRandomSampler, DataLoader\n",
    "from torchvision import datasets, models, transforms\n",
    "from torchvision.transforms import ToTensor, Compose\n",
    "from torchvision.datasets import MNIST\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "import torch.nn as nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "classes = list(range(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the training dataset\n",
    "trainset = MNIST(root='data/', train=True,\n",
    "                      transform=ToTensor(), download=True)\n",
    "\n",
    "# Download the test dataset\n",
    "testset = MNIST(root='data/', train=False,\n",
    "                     transform=ToTensor(), download=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.8 * len(trainset))\n",
    "val_size = len(trainset) - train_size\n",
    "trainset, valset = torch.utils.data.random_split(trainset,\n",
    "                                                 [train_size, val_size])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    trainset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    valset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    testset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48000, 12000, 10000)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trainset), len(valset), len(testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = trainset.dataset.targets[trainset.indices].numpy()\n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "_idx, subset_idx = next(split.split(trainset.indices, labels))\n",
    "new_indices = list(np.array(trainset.indices)[subset_idx])\n",
    "subset_sampler = SubsetRandomSampler(new_indices)\n",
    "\n",
    "subset_loader = DataLoader(trainset.dataset, batch_size=64, sampler=subset_sampler)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48000, 9600, 150, 48000)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels), len(new_indices), len(subset_loader), len(trainset.indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels=1, out_channels=10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            in_channels=10, out_channels=20, kernel_size=2)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        \n",
    "        self.fc1 = nn.Linear(in_features=20*5*5, out_features=50)\n",
    "        self.fc2 = nn.Linear(in_features=50, out_features=10)\n",
    "\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "\n",
    "        x = x.flatten(start_dim=1)\n",
    "\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "def train(model, train_loader, subset_loader, transform, val_loader, criterion, optimizer, epochs=1):\n",
    "\n",
    "    train_loss = []\n",
    "    val_loss = []\n",
    "    train_acc = []\n",
    "    val_acc = []\n",
    "    t = tqdm(total=epochs*(len(train_loader)+len(subset_loader)), desc=\"Epoch 0/{}\".format(epochs))\n",
    "    for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "        t.set_description(\"Epoch {}/{}\".format(epoch, epochs))\n",
    "        t.refresh()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "\n",
    "        for i, (inputs, labels) in enumerate(train_loader, 0):\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            outputs = torch.argmax(outputs, dim=1)\n",
    "            correct += (outputs == labels).float().sum()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            t.update(1)\n",
    "        for i, (inputs, labels) in enumerate(subset_loader, 0):\n",
    "            inputs = transform(inputs)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            outputs = torch.argmax(outputs, dim=1)\n",
    "            correct += (outputs == labels).float().sum()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            t.update(1)\n",
    "\n",
    "        loss = running_loss / len(train_loader)\n",
    "        accuracy = 100*correct/len(train_loader.dataset)\n",
    "        train_loss.append(loss)\n",
    "        train_acc.append(accuracy)\n",
    "        correct = 0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for i, (inputs, labels) in enumerate(val_loader, 0):\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                outputs = torch.argmax(outputs, dim=1)\n",
    "                correct += (outputs == labels).float().sum()\n",
    "\n",
    "                running_loss += loss.item()\n",
    "            loss = running_loss / len(val_loader)\n",
    "            accuracy = 100*correct/len(val_loader.dataset)\n",
    "            val_loss.append(loss)\n",
    "            val_acc.append(accuracy)\n",
    "\n",
    "        model.train()\n",
    "    t.refresh()\n",
    "    t.close()\n",
    "\n",
    "    print('Finished Training')\n",
    "    return train_loss, val_loss, train_acc, val_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms_arr = [\n",
    "    Compose([transforms.Resize((32, 32)),transforms.RandomCrop(28)]),\n",
    "    transforms.RandomHorizontalFlip(p=1),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.GaussianBlur(kernel_size=3, sigma=(0.1, 2.0)),\n",
    "]\n",
    "transforms_names = [\n",
    "    'Resize',\n",
    "    'LeftRightFlip',\n",
    "    'Rotation',\n",
    "    'GaussianNoise',\n",
    "]\n",
    "transforms_combined = Compose(\n",
    "    transforms_arr+[transforms.ToTensor()])\n",
    "\n",
    "transforms_arr.append(transforms_combined)\n",
    "transforms_names.append('Combined')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "applying Resize\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "for transform,transform_name in zip(transforms_arr, transforms_names):\n",
    "    print('applying',transform_name)\n",
    "    model = CNN()\n",
    "    train_loss, val_loss, train_acc, val_acc = train(model, train_loader, subset_loader, transform, val_loader, nn.CrossEntropyLoss(\n",
    "    ), optim.Adam(model.parameters(), lr=0.001), epochs=10)\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(10, 5), squeeze=False)\n",
    "    axs[0, 0].plot(train_loss, label='train')\n",
    "    axs[0, 0].plot(val_loss, label='val')\n",
    "    axs[0, 0].set_title('Loss vs Epochs')\n",
    "    axs[0, 0].legend()\n",
    "    # axs[0, 0].show()\n",
    "\n",
    "\n",
    "    axs[0, 1].plot(train_acc, label='train')\n",
    "    axs[0, 1].plot(val_acc, label='val')\n",
    "    axs[0, 1].set_title('Accuracy vs Epochs')\n",
    "    axs[0, 1].legend()\n",
    "    plt.show()\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3ff9dc6caf1e154d5edc32ebcbf2545cdc1e8c9297072b7268dfd34d4cb9c77b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
