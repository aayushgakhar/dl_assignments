{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, SubsetRandomSampler, DataLoader\n",
    "from torchvision import datasets, models, transforms\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision.datasets import MNIST\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "import torch.nn as nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "classes = list(range(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the training dataset\n",
    "trainset = MNIST(root='data/', train=True,\n",
    "                      transform=ToTensor(), download=True)\n",
    "\n",
    "# Download the test dataset\n",
    "testset = MNIST(root='data/', train=False,\n",
    "                     transform=ToTensor(), download=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.8 * len(trainset))\n",
    "val_size = len(trainset) - train_size\n",
    "trainset, valset = torch.utils.data.random_split(trainset,\n",
    "                                                 [train_size, val_size])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    trainset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    valset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    testset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48000, 12000, 10000)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trainset), len(valset), len(testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = trainset.dataset.targets[trainset.indices].numpy()\n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "_idx, subset_idx = next(split.split(trainset.indices, labels))\n",
    "new_indices = list(np.array(trainset.indices)[subset_idx])\n",
    "subset_sampler = SubsetRandomSampler(new_indices)\n",
    "\n",
    "subset_loader = DataLoader(trainset.dataset, batch_size=64, sampler=subset_sampler)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48000, 9600, 150, 48000)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels), len(new_indices), len(subset_loader), len(trainset.indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels=1, out_channels=10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            in_channels=10, out_channels=20, kernel_size=2)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        \n",
    "        self.fc1 = nn.Linear(in_features=20*5*5, out_features=50)\n",
    "        self.fc2 = nn.Linear(in_features=50, out_features=10)\n",
    "\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "\n",
    "        x = x.flatten(start_dim=1)\n",
    "\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        x = self.softmax(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms_arr = [\n",
    "    transforms.Resize((32, 32)),\n",
    "    transforms.RandomHorizontalFlip(p=1),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.GaussianBlur(kernel_size=3, sigma=(0.1, 2.0)),\n",
    "]\n",
    "transforms_names = [\n",
    "    'Resize',\n",
    "    'LeftRightFlip',\n",
    "    'Rotation',\n",
    "    'GaussianNoise',\n",
    "]\n",
    "transforms_combined = torchvision.transforms.Compose(\n",
    "    transforms_arr+[transforms.ToTensor()])\n",
    "\n",
    "transforms_arr.append(transforms_combined)\n",
    "transforms_names.append('Combined')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "applying Resize\n",
      "applying LeftRightFlip\n",
      "applying Rotation\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m augmented_subset \u001b[39m=\u001b[39m []\n\u001b[1;32m      4\u001b[0m \u001b[39mfor\u001b[39;00m image, label \u001b[39min\u001b[39;00m subset_loader:\n\u001b[0;32m----> 5\u001b[0m     augmented_subset\u001b[39m.\u001b[39mappend(transform(image))\n",
      "File \u001b[0;32m~/dl_assignments/.env/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/dl_assignments/.env/lib/python3.8/site-packages/torchvision/transforms/transforms.py:1353\u001b[0m, in \u001b[0;36mRandomRotation.forward\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m   1350\u001b[0m         fill \u001b[39m=\u001b[39m [\u001b[39mfloat\u001b[39m(f) \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m fill]\n\u001b[1;32m   1351\u001b[0m angle \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_params(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdegrees)\n\u001b[0;32m-> 1353\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mrotate(img, angle, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minterpolation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mexpand, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcenter, fill)\n",
      "File \u001b[0;32m~/dl_assignments/.env/lib/python3.8/site-packages/torchvision/transforms/functional.py:1118\u001b[0m, in \u001b[0;36mrotate\u001b[0;34m(img, angle, interpolation, expand, center, fill)\u001b[0m\n\u001b[1;32m   1115\u001b[0m \u001b[39m# due to current incoherence of rotation angle direction between affine and rotate implementations\u001b[39;00m\n\u001b[1;32m   1116\u001b[0m \u001b[39m# we need to set -angle.\u001b[39;00m\n\u001b[1;32m   1117\u001b[0m matrix \u001b[39m=\u001b[39m _get_inverse_affine_matrix(center_f, \u001b[39m-\u001b[39mangle, [\u001b[39m0.0\u001b[39m, \u001b[39m0.0\u001b[39m], \u001b[39m1.0\u001b[39m, [\u001b[39m0.0\u001b[39m, \u001b[39m0.0\u001b[39m])\n\u001b[0;32m-> 1118\u001b[0m \u001b[39mreturn\u001b[39;00m F_t\u001b[39m.\u001b[39;49mrotate(img, matrix\u001b[39m=\u001b[39;49mmatrix, interpolation\u001b[39m=\u001b[39;49minterpolation\u001b[39m.\u001b[39;49mvalue, expand\u001b[39m=\u001b[39;49mexpand, fill\u001b[39m=\u001b[39;49mfill)\n",
      "File \u001b[0;32m~/dl_assignments/.env/lib/python3.8/site-packages/torchvision/transforms/functional_tensor.py:671\u001b[0m, in \u001b[0;36mrotate\u001b[0;34m(img, matrix, interpolation, expand, fill)\u001b[0m\n\u001b[1;32m    668\u001b[0m \u001b[39m# grid will be generated on the same device as theta and img\u001b[39;00m\n\u001b[1;32m    669\u001b[0m grid \u001b[39m=\u001b[39m _gen_affine_grid(theta, w\u001b[39m=\u001b[39mw, h\u001b[39m=\u001b[39mh, ow\u001b[39m=\u001b[39mow, oh\u001b[39m=\u001b[39moh)\n\u001b[0;32m--> 671\u001b[0m \u001b[39mreturn\u001b[39;00m _apply_grid_transform(img, grid, interpolation, fill\u001b[39m=\u001b[39;49mfill)\n",
      "File \u001b[0;32m~/dl_assignments/.env/lib/python3.8/site-packages/torchvision/transforms/functional_tensor.py:562\u001b[0m, in \u001b[0;36m_apply_grid_transform\u001b[0;34m(img, grid, mode, fill)\u001b[0m\n\u001b[1;32m    559\u001b[0m     mask \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mones((img\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], \u001b[39m1\u001b[39m, img\u001b[39m.\u001b[39mshape[\u001b[39m2\u001b[39m], img\u001b[39m.\u001b[39mshape[\u001b[39m3\u001b[39m]), dtype\u001b[39m=\u001b[39mimg\u001b[39m.\u001b[39mdtype, device\u001b[39m=\u001b[39mimg\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m    560\u001b[0m     img \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat((img, mask), dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m--> 562\u001b[0m img \u001b[39m=\u001b[39m grid_sample(img, grid, mode\u001b[39m=\u001b[39;49mmode, padding_mode\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mzeros\u001b[39;49m\u001b[39m\"\u001b[39;49m, align_corners\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m    564\u001b[0m \u001b[39m# Fill with required color\u001b[39;00m\n\u001b[1;32m    565\u001b[0m \u001b[39mif\u001b[39;00m fill \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/dl_assignments/.env/lib/python3.8/site-packages/torch/nn/functional.py:4235\u001b[0m, in \u001b[0;36mgrid_sample\u001b[0;34m(input, grid, mode, padding_mode, align_corners)\u001b[0m\n\u001b[1;32m   4227\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m   4228\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mDefault grid_sample and affine_grid behavior has changed \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   4229\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mto align_corners=False since 1.3.0. Please specify \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   4230\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39malign_corners=True if the old behavior is desired. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   4231\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mSee the documentation of grid_sample for details.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   4232\u001b[0m     )\n\u001b[1;32m   4233\u001b[0m     align_corners \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m-> 4235\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mgrid_sampler(\u001b[39minput\u001b[39;49m, grid, mode_enum, padding_mode_enum, align_corners)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for transform,transform_name in zip(transforms_arr, transforms_names):\n",
    "    print('applying',transform_name)\n",
    "    augmented_subset = []\n",
    "    for image, label in subset_loader:\n",
    "        augmented_subset.append(transform(image))\n",
    "        model = CNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3ff9dc6caf1e154d5edc32ebcbf2545cdc1e8c9297072b7268dfd34d4cb9c77b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
